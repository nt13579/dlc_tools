"""
deeplabcut tools for data analysis

Created by Nicholas Thomas
Last Edited 7/29/19

Notes for points of improvement. Change in paramaters to pass in. Accepting files, or config file
and going from there

filepath is currently a mandatory parameter

function list:
quantifyErrors(trueLabels, testLabels, threshold, filepath, split=False, getParams=False)
testThresholds(trueLabels, testLabels, thresholds, filepath)
plotErrors(trueLabels, testLabels, thresholds, filepath, snapshots=[] , save=True, normalize=True)
"""

import numpy as np
import pandas as pd
import os
import math
import time
import sys
from pathlib import Path
import collections
import yaml

#Matplotlib Configuration
import seaborn as sb
import matplotlib
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
matplotlib.use('WxAgg')

import sklearn.metrics as sk

#True Positive, True Negative, False Positive, False Negative
errorTypes = ['TP', 'TN', 'FP', 'FN']

def quantifyErrors(trueLabels, testLabels,  filepath, threshold=.1, filter=True):
    """Short summary.

    Calculates the errors on the deeplabcut training and test data. Classifies each label as True
    Positive (TP), True Negative (TN), False Positive (FP), or False Negative (FN). For each true
    Positive the distance between the true (manually labeled) points and the dlc labeled points is
    calculated

    Parameters
    ----------
    trueLabels : type = panda, or path to hdf file
        Manually labeled data. This is the "ground" truth against which the test labels
        are compared to. Includes just an x and y coordinate for each label in each frame,
        the coordinate value is Nan if the label is not included in that frame.
    testLabels : type = panda, or path to hdf file
        The labels generated by deeplabcut. includes an x,y, and confidence for each label
        for each frame.
    threshold : type = double
        The pcutoff or threshold above which labels are included for a frame. Value must be
        between 0 and 1. Default is .1
    filepath : type = string
        The file path to the folder of all the frames which include test or train in their
        labels.
    getParams : type = boolean
        If true, returns a dictionary with parameters
    filter : type = boolean
        If true, will filter the test labels. Any True positive label with a distance greater than
        (mean + 1.5*IQR) will be reclassified as a false positive and its corresponding
        distance set to nan - IQR is interquartile range. The mean and IQR are calculated for each
        individual label

    Returns
    -------
    results : type = pandas
        Columns: [Type] [[Label Names], [ErrorType, Confidence, Distance]]
     frame by frame description of the data and errors (Raw Error Analysis)

    SnS: by label description of the data (Sensitivity and Specificity Matrix)

    (Optional Returns)
    parameters: {labels: [] of '', numLabels: int, numFrames: int }

    """
    np.warnings.filterwarnings('ignore')

    trueLabels = checkInput(trueLabels)
    testLabels = checkInput(testLabels)

    #Ensures that the threshold is between 0 and 1
    if(threshold > 1 or threshold < 0):
        raise Exception('threshold must be between the values of 0 and 1 inclusive;'
                        ' you passed in threshold={}'.format(threshold)
                        )

    #Parameters
    maxPixelCutoff = 50

    labels = getLabelNames(trueLabels)
    numLabels = len(labels) #Number of Labels
    numFrames = getFrameCount(filepath)['Total'] #Number of frames in the dataset

    frameType = getFrameType(filepath) #Dictionary of frame names with Train or Test identifier
    header = pd.MultiIndex.from_product([labels, ['Error Type','Confidence', 'Distance']])
    indices = list(frameType.keys())
    results = pd.DataFrame(index = indices, columns = header)
    results.insert(0, 'Type',frameType.values())
    results = results.sort_index() #The results dataframe is nan except for the Type (Train or Test)

    #Convert Pandas to np array
    trueVals = trueLabels.values
    testVals = testLabels.values

    testConfidences = testVals[:,2::3] #confidence for each test label
    testValsXY = np.delete(testVals, list(range(2,testVals.shape[1], 3)), axis=1) #XY coords
    thresholdTest = (testConfidences > threshold) #Indices of labels greater than threshold
    labelTest = pd.isnull(trueVals)[:,::2] #Indices of labels that should not be included

    #when a label is nan it means it should not be plotted (it should be a negative)
    tp = np.multiply(thresholdTest,~labelTest)      #greater than threshold, should be plotted
    fp = np.multiply(thresholdTest, labelTest)      #greater than threshold, should not be plotted
    fn = np.multiply(~thresholdTest, ~labelTest)    #lower than threshold, should be plotted
    tn = np.multiply(~thresholdTest, labelTest)     #lower than threshold, should not be plotted

    #Initializing error Matrix for each label across each frame
    errors = np.empty((numFrames, numLabels), dtype = object)
    errors[tp] = "TP" #True Positive
    errors[fp] = "FP" #False Positive
    errors[fn] = "FN" #False Negatives
    errors[tn] = "TN" #True Negatives

    #This clusters takes sequential xy values and makes them pairs for true positive labels.
    #Original shape: (numFrames, numLables*2)
    XY1 = trueVals.reshape(numFrames, numLabels, 2)[tp]
    XY2 = testValsXY.reshape(numFrames, numLabels, 2)[tp]
    dist = []
    if(np.shape(XY1)[0] != 0):
        dist = sk.pairwise.paired_distances(XY1,XY2) #1D array of length # of True Positives
    dist2 = np.full(numFrames*numLabels, np.nan)
    dist2[tp.flatten()] = dist  #Makes tp 1D in order to retrieve all tp indices to set to
                                #corresponding distance

    dist2[dist2>maxPixelCutoff] = math.nan #hard Filter Limit
    dist2 = dist2.reshape(numFrames,numLabels)

    #Each label has three values: Error Type, Confidence, and distance. Thus the indexing by three.
    #Errors start at column 1 as column 0 is the training or test identification
    results.iloc[:,1::3] = errors
    results.iloc[:,2::3] = testConfidences
    results.iloc[:,3::3] = dist2

    if filter:
        resVals = results[results['Type']=='Test'].iloc[:,3::3]

        IQR = resVals.quantile(.75) - resVals.quantile(.25)
        distThresholds =resVals.mean() + 1.5*IQR

        resBool = (resVals > distThresholds).values
        resVals = resVals.values
        resVals[resBool] = np.nan
        results.ix[results['Type'] == 'Test', 3::3] = resVals
        err = results.ix[results['Type'] == 'Test', 1::3].values
        err[resBool] = 'FP'
        results.ix[results['Type'] == 'Test', 1::3] = err
        resVals = results[results['Type']=='Training'].iloc[:,3::3]

        IQR = resVals.quantile(.75) - resVals.quantile(.25)
        distThresholds =resVals.mean() + 1.5*IQR

        resBool = (resVals > distThresholds).values
        resVals = resVals.values
        resVals[resBool] = np.nan
        results.ix[results['Type'] == 'Training', 3::3] = resVals
        err = results.ix[results['Type'] == 'Training', 1::3].values
        err[resBool] = 'FP'
        results.ix[results['Type'] == 'Training', 1::3] = err

    header = pd.MultiIndex.from_product([['Training', 'Test'], errorTypes])
    SnS = pd.DataFrame(index = labels, columns = header) #Sns: Specificity and Sensitivity
    SnS = SnS.fillna(0)

    testErr = results[results['Type'] == 'Test'].iloc[:,1::3]
    trainErr = results[results['Type'] == 'Training'].iloc[:,1::3]
    for i, errorType in enumerate(errorTypes):
        SnS.iloc[:,i] = (trainErr == errorType).sum().values
        SnS.iloc[:,i+4] = (testErr == errorType).sum().values

    return results, SnS


def testThresholds(trueLabels, testLabels, filepath, thresholds = [], filter=True):
    """Short summary.

    Calculates the errors at several different thresholds

    Parameters
    ----------
    trueLabels : type = panda, or path to hdf file
        Manually labeled data. This is the "ground" truth against which the test labels
        are compared to. Includes just an x and y coordinate for each label in each frame,
        the coordinate value is Nan if the label is not included in that frame.
    testLabels : type = panda, or path to hdf file
        The labels generated by deeplabcut. includes an x,y, and confidence for each label
        for each frame.
    threshold : type = [] of doubles
        An array of thresholds above which labels are included for a frame. Values must be
        between 0 and 1.
    filepath : type = string
        The file path to the folder of all the frames which include test or train in their
        labels.

    Returns
    -------
    avgOccurences (panda): the average of the occurences of error type and respective confidences
                          taken across all labels at a threshold
    distances (panda): for each label, calculates the average distance and standard deviations
                       between the manually labeled points and the dlc labled points
    snapshots ([] of pandas): an array of SnS pandas at the different thresholds

    """

    checkThresholds(thresholds)

    header = pd.MultiIndex.from_product([['Training', 'Test'], errorTypes])
    avgOccurences = pd.DataFrame(index = thresholds, columns = header)

    snapshots = []

    labels = getLabelNames(trueLabels)
    subHeader = ['Average'] + labels
    header = pd.MultiIndex.from_product([['Training', 'Test'], subHeader,
                                         ['Distance', 'Standar Deviation']])
    distances = pd.DataFrame(index = thresholds, columns = header)

    for i, t in enumerate(thresholds):
        results, SnS = quantifyErrors(trueLabels, testLabels, filepath, threshold=t, filter=filter)
        avgOccurences.iloc[i] = list(round(SnS.mean(), 4))
        tempTrainData = results[results['Type'] == 'Training'].iloc[:,3::3] #gets Train distances
        tempTestData = results[results['Type'] == 'Test'].iloc[:,3::3] #gets Test distances
        trainDist = tempTrainData.mean().values.tolist()
        testDist = tempTestData.mean().values.tolist()
        trainVar = tempTrainData.std().values.tolist()
        testVar = tempTestData.std().values.tolist()
        trainDist.insert(0, np.mean(trainDist))
        testDist.insert(0, np.mean(testDist))
        trainVar.insert(0, np.nanstd(tempTrainData, dtype = float))
        testVar.insert(0, np.nanstd(tempTestData, dtype = float))
        distances.iloc[i, ::2] = trainDist+testDist
        distances.iloc[i,1::2] = trainVar+testVar
        snapshots.append(SnS)


    return avgOccurences, distances, snapshots

#Gets the trueLabels, testLabels, filepath based on the PCF and desired shuffle number. Ensures that
#the trueLabels h5 for each video are appended in the correct order matching the test Labels.
def getDataFromPCF(pcf, shuffle):
    #Open and read PCF and convert to dictionary
    stream = open(pcf, 'r')
    dictionary = yaml.load(stream)

    iterNum = str(dictionary['iteration']) #Training Iteration Number, set in config file
    #numFramesPerCam = dictionary['numframes2pick']

    testDir = os.path.dirname(pcf) #Deep Lab Cut Main Experiment folder
    trueDir = testDir
    testDir = testDir+ '/evaluation-results' #Evaluation Folder
    evalList = os.listdir(testDir)
    iterFolder = 'iteration-' + iterNum #Specific iteration folder

    #Checks that the iteration in the config file has already been evaluated and exists
    if (iterFolder) in evalList:
        testDir+= '/' + iterFolder
    else:
        raise Exception('Iteration Number is not found. Network has'
                        'not been evaluated for iteration {}'.format(iterNum)
                        )

    shuffles = os.listdir(testDir) #Gets all possible shuffles to evaluate against

    #Checks that the passed in shuffle exists
    for s in shuffles:
        if s.endswith(str(shuffle)):
            testDir+= '/' + s    #Specific shuffle number
        else:
            raise Exception('Shuffle Number is not found. Network has'
                            'not been evaluated for shuffle {}'.format(shuffle)
                            )


    for file in os.listdir(testDir):
        if file.endswith('h5') & ('snapshot' in file):
            resultFile = file  #Gets the h5 file with the evaluation results
        elif 'Images' in file:
            filepath = testDir + '/' + file #get the directory with all the images
                                            #classified as train or test

    testLabels = pd.read_hdf(testDir + '/' + resultFile)
    labelNames = getLabelNames(testLabels)

    tempTrueLabels = {}
    trueDir +=  '/' + 'labeled-data'
    videos = os.listdir(trueDir) #All Vid Folders with Manual annotations

    #Parse through vid folders and extract the h5 files for each manually labeled video
    for i, path in enumerate(videos):
        tempPath = trueDir+ '/' + path
        file = [f for f in os.listdir(tempPath) if f.endswith('.h5')] #h5 file for manual labeling
        data = pd.read_hdf(tempPath + '/' + file[0])
        tempLabels = getLabelNames(data)

        #POSSIBLY TEMPORARY: Removes columns not for labels not found in results labels, this is
        #caused by removing a label from the config file after starting labeling
        for label in tempLabels:
            if label not in labelNames:
                data = data.drop([label], axis = 1, level = 1)

        tempTrueLabels[i] = data;

    #Evenly samples the eval results (testLabels) frames  with number of points equal to number of
    #videos and gets the frame names for those indices. Each video will have one frame in the vector
    vidOrder = testLabels.index.values[np.linspace(0, testLabels.shape[0] - 1, len(videos), dtype = int)]
    tempOrder = [0,0,0]

    #Matches the indices of the videos vector to the order of the videos in the testLabels data frame
    for i in range(len(videos)):
        for j, l in enumerate(videos):
            if(l in vidOrder[i]):
                tempOrder[i] = j

    trueLabels = pd.DataFrame()

    for n in tempOrder:
        if trueLabels.empty:
            trueLabels = tempTrueLabels[n].reindex(labelNames, axis = 1, level = 1)
        else:
            trueLabels = trueLabels.append(tempTrueLabels[n].reindex(labelNames, axis = 1, level = 1))

    return(trueLabels, testLabels, filepath)



#Returns list of label names in the same order they are in the file or pandas
def getLabelNames(data):
    data = checkInput(data)
    labels = data.columns.get_level_values(1).tolist()
    labels = list(collections.OrderedDict.fromkeys(labels))
    return labels

#Returns a dictionary with test frame count, training frame count, and total frame count.
def getFrameCount(filepath):
    frameType = getFrameType(filepath)
    testCount = list(frameType.values()).count('Test')
    trainCount = list(frameType.values()).count('Training')

    return {'Test':testCount, 'Training':trainCount, 'Total':(testCount+trainCount)}

#Returns a dictionary of frame names with their corresponding type (Test or Training). Frame names
#are shortened by default to their image number ####NEEDS UPDATING
def getFrameType(filepath, shortenName = True):
    frameNames = os.listdir(filepath)
    checkFileType = (lambda x: x[-3:] == 'png')
    if not all(list(map(checkFileType, frameNames))):
        raise Exception('Files are not all of type .png, check file path to ensure it references '
                        'the correct folder')

    frameType = {}
    for f in frameNames:
        #frame name ex: Type-VidName-img####.png -> ['Type', VidName, img####]
        frameName = f.split('-')
        if shortenName:
            frameType[frameName[1] +'-'+ frameName[-1][:-4]] = frameName[0]
        else:
            frameType[frameName[1]+'-'+frameName[2]] = frameName[0]
    return frameType


#Checks
#-------------------------------------------------------------------------------------------------#

def checkThresholds(thresholds):
        #Ensures treshold type
        if not isinstance(thresholds, (list, np.ndarray) ):
            raise Exception('thresholds was of the incorrect type. You passed in a [{}] when it should '
                            'have been a list or np.ndarray'.format(str(type(thresholds))[8:-2]))

        #Ensures thresholds is an nparray for boolean checking. Also sorts values in ascending order
        thresholds = np.sort(np.asarray(thresholds))

        #Checks to make sure there is more than 1 value in thresholds
        if thresholds.size == 1:
            raise Exception('thresholds is a singular value of {}, not an array or list of multiple '
                            'values'.format(thresholds))

        if thresholds.size == 0:
            raise Exception('thresholds is empty, must input an array or list of values for thresholds')

        #Checks that the array is not multi dimensional
        if thresholds.shape != thresholds.flatten().shape:
            raise Exception('you passed in a multidimensional array instead of a 1D list or array')

        #Checks if any thresholds are repeated
        for i,t in enumerate(thresholds[:-1]):
            if t == thresholds[i+1]:
                raise Exception('{} is a repeated value. Tresholds should only contain unique values '
                                'between 0 and 1'.format(t))

        t_Below = thresholds[(thresholds<0)] #values less than 0
        t_Above = thresholds[(thresholds>1)] #values greater than 1

        #Ensures no threshold value is below 0
        if t_Below.size>0:
            raise Exception('{} are below threshold minimum of 0'.format(t_Below))

        #Ensures no threshold value is above 1
        if t_Above.size>0:
            raise Exception('{} are above threshold maximum of 1'.format(t_Above))



#Ensures input data is of the correct data type: panda or Path (windows) or string (Ubuntu). The
#file must be either a h5 or csv. If data is not a panda, converts to a panda and returns the
#dataFrame.
def checkInput(data):
    if not isinstance(data, (pd.DataFrame, Path, str)):
        raise Exception('Input data must be either a panda dataframe or the h5 output files created'
                        ' when training DLC. You passed in a {}'.format(str(type(data)))
                        )

    if not isinstance(data, pd.DataFrame):
        if data[-2:] == 'h5':
            data = pd.read_hdf(data)
        elif data[-3:] == 'csv':
            data = pd.read_csv(data)
        else:
            raise Exception('File type must be either .h5 or .csv. You passed in an incompatible '
                            'file type')

    return data
